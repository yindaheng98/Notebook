# [Torch-Pruning](https://github.com/VainF/Torch-Pruning)è§£æ

## åº•å±‚è°ƒç”¨

```python
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))

import torch
import torch.nn as nn
import torch.nn.functional as F

import torch_pruning as tp
from typing import Sequence

############
# Customize your layer
#
class CustomizedLayer(nn.Module):
    def __init__(self, in_dim):
        super().__init__()
        self.in_dim = in_dim
        self.scale = nn.Parameter(torch.Tensor(self.in_dim))
        self.bias = nn.Parameter(torch.Tensor(self.in_dim))
        self.fc = nn.Linear(self.in_dim, self.in_dim)
    
    def forward(self, x):
        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()
        x = torch.div(x, norm)
        return self.fc(x * self.scale + self.bias)

    def __repr__(self):
        return "CustomizedLayer(in_dim=%d)"%(self.in_dim)

class FullyConnectedNet(nn.Module):
    """https://github.com/VainF/Torch-Pruning/issues/21"""
    def __init__(self, input_size, num_classes, HIDDEN_UNITS):
        super().__init__()
        self.fc1 = nn.Linear(input_size, HIDDEN_UNITS)
        self.customized_layer = CustomizedLayer(HIDDEN_UNITS)
        self.fc2 = nn.Linear(HIDDEN_UNITS, num_classes)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.customized_layer(x)
        y_hat = self.fc2(x)
        return y_hat

############################
# Implement your pruning function for the customized layer
# You should implement the following class fucntions:
# 1. prune_out_channels
# 2. prune_in_channels
# 3. get_out_channels
# 4. get_in_channels

class MyPruner(tp.pruner.BasePruningFunc):

    def prune_out_channels(self, layer: CustomizedLayer, idxs: Sequence[int]) -> nn.Module: 
        keep_idxs = list(set(range(layer.in_dim)) - set(idxs))
        keep_idxs.sort()
        layer.in_dim = layer.in_dim-len(idxs)
        layer.scale = torch.nn.Parameter(layer.scale.data.clone()[keep_idxs])
        layer.bias = torch.nn.Parameter(layer.bias.data.clone()[keep_idxs])
        tp.prune_linear_in_channels(layer.fc, idxs)
        tp.prune_linear_out_channels(layer.fc, idxs)
        return layer

    def get_out_channels(self, layer):
        return self.in_dim
    
    # identical functions
    prune_in_channels = prune_out_channels
    get_in_channels = get_out_channels
        
model = FullyConnectedNet(128, 10, 256)

DG = tp.DependencyGraph()

# 1. Register your customized layer
my_pruner = MyPruner()
DG.register_customized_layer(
    CustomizedLayer, 
    my_pruner)

# 2. Build dependency graph
DG.build_dependency(model, example_inputs=torch.randn(1,128))

# 3. get a pruning group according to the dependency graph. idxs is the indices of pruned filters.
pruning_group = DG.get_pruning_group( model.fc1, tp.prune_linear_out_channels, idxs=[0, 1, 6] )
print(pruning_group)

# 4. execute this group (prune the model)
pruning_group.exec()
print("The pruned model:\n", model)
print("Output: ", model(torch.randn(1,128)).shape)

assert model.fc1.out_features==253 and model.customized_layer.in_dim==253 and model.fc2.in_features==253
```

`DG = tp.DependencyGraph()`æ˜¯æ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒæ¨¡å—ï¼Œçœ‹å®ƒçš„è°ƒç”¨æµç¨‹ä¹Ÿå°±çœ‹æ‡‚äº†æ•´ä¸ª`Torch-Pruning`çš„è£å‰ªè¿‡ç¨‹ã€‚

ä»`DG = tp.DependencyGraph()`å¼€å§‹çš„è°ƒç”¨ä»ä¸Šå¾€ä¸‹çœ‹ï¼Œå¯ä»¥çœ‹è§`DG.register_customized_layer`ã€`DG.build_dependency`ã€`pruning_group = DG.get_pruning_group`å’Œ`pruning_group.exec()`ã€‚

### `DG.register_customized_layer`

`DG.register_customized_layer`ç”¨äºæ³¨å†Œâ€œè£å‰ªæ–¹å¼â€ã€‚â€œè£å‰ªæ–¹å¼â€ä¸Pytorchä¸­çš„å±‚ä¸€ä¸€å¯¹åº”

`DG = tp.DependencyGraph()`å†…éƒ¨å·²ç»æ³¨å†Œäº†ä¸€äº›å±‚çš„é»˜è®¤è£å‰ªæ–¹å¼ï¼ŒåŒ…æ‹¬å·ç§¯å±‚å’Œçº¿æ€§å±‚ç­‰ï¼š

```python
PrunerBox = {
    ops.OPTYPE.CONV: ConvPruner(),
    ops.OPTYPE.LINEAR: LinearPruner(),
    ops.OPTYPE.BN: BatchnormPruner(),
    ops.OPTYPE.DEPTHWISE_CONV: DepthwiseConvPruner(),
    ops.OPTYPE.PRELU: PReLUPruner(),
    ops.OPTYPE.LN: LayernormPruner(),
    ops.OPTYPE.EMBED: EmbeddingPruner(),
    ops.OPTYPE.PARAMETER: ParameterPruner(),
    ops.OPTYPE.MHA: MultiheadAttentionPruner(),
    ops.OPTYPE.LSTM: LSTMPruner()
}
```

```python
_dummy_pruners = {
    ops.OPTYPE.CONCAT: ops.ConcatPruner(),
    ops.OPTYPE.SPLIT: ops.SplitPruner(),
    ops.OPTYPE.ELEMENTWISE: ops.ElementWisePruner(),
    ops.OPTYPE.CUSTOMIZED: None,
}
```

### `DG.build_dependency`

`DG.build_dependency`ç”¨äºè§£ææ¨¡å‹ä¸­å±‚ä¹‹é—´çš„è°ƒç”¨å…³ç³»ï¼Œå³è§£æ`torch.nn.Module.forward`ä¸­çš„å†…å®¹ã€‚

é‡‡å–çš„æ–¹å¼æ˜¯ç”¨ä¸€ä¸ªæ ·ä¾‹è¾“å…¥æ‰§è¡Œæ¨æ–­è¿‡ç¨‹ï¼Œåœ¨æ¨æ–­è¿‡ç¨‹è¿›è¡Œtraceã€‚
å…·ä½“çš„traceæ–¹æ¡ˆåœ¨`DG.build_dependency._trace`å‡½æ•°ä¸­ã€‚
ç®€è¨€ä¹‹ï¼Œå°±æ˜¯é€šè¿‡`torch.nn.Module.register_forward_hook`æ³¨å†Œhookï¼Œä»è€Œåœ¨æ¯ä¸ª`forward`å‡½æ•°è¢«è°ƒç”¨æ—¶è®°å½•ä¸‹è°ƒç”¨é¡ºåºã€‚

å…·ä½“æ¥è¯´ï¼Œ`DG.build_dependency._trace`å‡½æ•°traceå‡ºçš„è°ƒç”¨é¡ºåºåŒ…æ‹¬ä¸¤æ–¹é¢ï¼šè¾“å…¥æ¥è‡ªå“ªäº›å±‚ã€è¾“å‡ºåˆ°å“ªäº›å±‚ã€‚
å¾—çŸ¥è¿™äº›ä¿¡æ¯åï¼Œ`DG.build_dependency`ä¼šè°ƒç”¨`DG._build_dependency`ï¼Œè¿™ä¸ªå‡½æ•°å°†æ¯ä¸€ä¸ªå±‚ä¸å±‚ä¹‹é—´çš„è°ƒç”¨é¡ºåºï¼ˆxå±‚çš„è¾“å‡ºåˆ°yå±‚çš„è¾“å…¥ï¼‰æ„å»ºä¸ºä¸€ä¸ª`tp.Dependency`ï¼ŒåŠ è¿›ç›¸å…³å±‚çš„`node.dependencies`ä¸­ï¼š
```python
    def _build_dependency(self, module2node):

        # There will be a dependency between two pruning operations if they:
        # 1) connects to each other in the computational graph or
        # 2) are equivalent, i.e., applied to the same layer and works in the same way.
        # Note that for some units like BN and PReLU, pruning output channels is equivalent to pruning output_channels
        # Rule 2) is designed for this case.

        for _, node in module2node.items():
            # Rule 1) - Input connections
            for in_node in node.inputs:
                handler = self.REGISTERED_PRUNERS.get(in_node.type)
                if handler is None:
                    handler = self.CUSTOMIZED_PRUNERS[in_node.class_type]
                handler = handler.prune_out_channels

                trigger = self.REGISTERED_PRUNERS.get(node.type)
                if trigger is None:
                    trigger = self.CUSTOMIZED_PRUNERS[node.class_type]
                trigger = trigger.prune_in_channels

                dep = Dependency(
                    trigger=trigger, handler=handler, source=node, target=in_node
                )
                node.dependencies.append(dep)

            # Rule 1) - Output connections
            for out_node in node.outputs:
                trigger = self.REGISTERED_PRUNERS.get(node.type)
                if trigger is None:
                    trigger = self.CUSTOMIZED_PRUNERS[node.class_type]
                trigger = trigger.prune_out_channels

                handler = self.REGISTERED_PRUNERS.get(out_node.type)
                if handler is None:
                    handler = self.CUSTOMIZED_PRUNERS[out_node.class_type]
                handler = handler.prune_in_channels

                dep = Dependency(
                    trigger=trigger, handler=handler, source=node, target=out_node
                )
                node.dependencies.append(dep)
......
```
çœ‹`tp.Dependency`çš„è¾“å…¥`trigger=trigger, handler=handler, source=node, target=out_node`ï¼Œå¾ˆæ˜æ˜¾è¿™è¡¨ç¤ºï¼šå½“`source`å±‚çš„è£å‰ªè¿‡ç¨‹`trigger`è¢«è°ƒç”¨æ—¶ï¼Œéœ€è¦è°ƒç”¨`target`å±‚çš„`handler`ã€‚

æ­¤å¤–ï¼Œè¿›ä¸€æ­¥çœ‹`REGISTERED_PRUNERS`å’Œ`CUSTOMIZED_PRUNERS`ï¼š
```python
class DependencyGraph(object):

    def __init__(self):
        _dummy_pruners = {
            ops.OPTYPE.CONCAT: ops.ConcatPruner(),
            ops.OPTYPE.SPLIT: ops.SplitPruner(),
            ops.OPTYPE.ELEMENTWISE: ops.ElementWisePruner(),
            ops.OPTYPE.CUSTOMIZED: None,
        }
        self.REGISTERED_PRUNERS = function.PrunerBox.copy()  # shallow copy
        self.REGISTERED_PRUNERS.update(_dummy_pruners)
        self.CUSTOMIZED_PRUNERS = {}
        self.IGNORED_LAYERS = []
......
    
    def register_customized_layer(
        self,
        layer_type,
        layer_pruner,
    ):
        """Register a customized layer for pruning.

        Args:
            layer_type (class): the type of layer
            pruner (tp.pruner.BasePruningFunc): a pruner for the given layer type.
        """
        self.CUSTOMIZED_PRUNERS[layer_type] = layer_pruner
......
```
å¯ä»¥å‘ç°å®ƒä»¬å®é™…ä¸Šéƒ½ç»§æ‰¿è‡ª`tp.pruner.BasePruningFunc`ï¼Œ`PrunerBox`é‡Œé¢çš„å‡ ä¸ªæ˜¯å·²å®ç°çš„`_dummy_pruners`é‡Œé¢çš„å‡ ä¸ªéƒ½æœªå®ç°ã€‚
æ‰€ä»¥å¾ˆæ˜æ˜¾ï¼Œåªæœ‰åœ¨`DG.register_customized_layer`æˆ–è€…å†…éƒ¨è‡ªå¸¦çš„ç»§æ‰¿è‡ª`tp.pruner.BasePruningFunc`çš„ç±»çš„ç±»æ–¹æ³•æ‰èƒ½è¢«ä½œä¸º`tp.Dependency`é‡Œçš„`trigger`å’Œ`handler`ã€‚
å†çœ‹è¿™ä¸ª`tp.pruner.BasePruningFunc`ï¼š
```python
class BasePruningFunc(ABC):
    TARGET_MODULES = ops.TORCH_OTHERS  # None

    def __init__(self, dim=1):
        self.dim = dim

    @abstractclassmethod
    def prune_out_channels(self, layer: nn.Module, idxs: Sequence[int]):
        raise NotImplementedError

    @abstractclassmethod
    def prune_in_channels(self, layer: nn.Module, idxs: Sequence[int]):
        raise NotImplementedError

    @abstractclassmethod
    def get_out_channels(self, layer: nn.Module):
        raise NotImplementedError

    @abstractclassmethod
    def get_in_channels(self, layer: nn.Module):
        raise NotImplementedError
        
......
```
è¿™ä¸å°±æ˜¯â€œè£è¾“å…¥â€å’Œâ€œè£è¾“å‡ºâ€å—ğŸ˜‚

æ‰€ä»¥**æ¯ä¸€å±‚çš„`node.dependencies`ä¸­å®é™…ä¸ŠåŒ…å«çš„éƒ½æ˜¯ï¼šâ€œtrigger=ä¸Šä¸€å±‚çš„è¾“å‡ºè¢«è£å‰ª, handler=è£å‰ªå½“å‰å±‚çš„è¾“å…¥â€æˆ–è€…â€œtrigger=ä¸‹ä¸€å±‚çš„è¾“å…¥è¢«è£å‰ª, handler=è£å‰ªå½“å‰å±‚çš„è¾“å‡ºâ€**ã€‚
çœ‹åˆ°è¿™å°±æ¸…æ™°å¤šäº†ï¼Œè¿™æ¡†æ¶ä¸»æ‰“çš„è‡ªåŠ¨è§£æä¾èµ–å…³ç³»å®Œæˆè£å‰ªçš„åŠŸèƒ½å½’æ ¹ç»“åº•å°±æ˜¯ä»¥è¿™ç§æ–¹å¼ç»„ç»‡çš„ã€‚

### `pruning_group = DG.get_pruning_group`

åœ¨`DG.build_dependency`ä¹‹åï¼Œæ¨¡å‹ä¸­æ¯ä¸ªModuleä¹‹é—´çš„è°ƒç”¨å…³ç³»å°±æ¸…æ¥šäº†ï¼Œäºæ˜¯è¾“å…¥ä»»æ„ä¸€ä¸ªè¦è£çš„â€œèŠ‚ç‚¹â€ï¼ˆè¾“å‡ºçŸ©é˜µä¸­çš„æŸä¸ªchannelï¼Œä¹Ÿå¯¹åº”å·ç§¯å±‚ä¸­çš„ä¸€ä¸ªå·ç§¯æ ¸ï¼‰éƒ½èƒ½çŸ¥é“è¯¥èŠ‚ç‚¹åœ¨æ¨¡å‹ä¸­çš„å‰åä¾èµ–å…³ç³»ï¼ˆä¾‹å¦‚è¯¥èŠ‚ç‚¹è¢«è£å‰ªå¯¼è‡´è¾“å‡ºå°‘äº†ä¸€ä¸ªchannelï¼Œä»¥ä¹‹ä½œä¸ºè¾“å…¥çš„æ‰€æœ‰å±‚ä¹Ÿéœ€è¦ç›¸åº”çš„è¿›è¡Œä¿®æ”¹ï¼‰ã€‚

`DG.get_pruning_group`å°±æ˜¯è¿™æ ·ä¸€ä¸ªæ ¹æ®è¾“å…¥çš„æŸä¸ªèŠ‚ç‚¹çš„è£å‰ªæ–¹æ¡ˆè¾“å‡ºæ•´ä½“è£å‰ªæ–¹æ¡ˆçš„å‡½æ•°ã€‚

å…¶è¾“å…¥ä¸ºè¦è£çš„å±‚`module: nn.Module`ã€è£å‰ªè¯¥èŠ‚ç‚¹çš„å‡½æ•°`pruning_fn: typing.Callable`å’Œè£å“ªäº›channel`idxs: typing.Union[list, tuple]`ã€‚
è¿™é‡Œçš„`pruning_fn`è™½ç„¶åªæ˜¯`typing.Callable`ï¼Œä½†ç»“åˆ`DG.build_dependency`çš„è§£æï¼Œå®é™…çš„ä¾èµ–å…³ç³»æ˜¯ç”±ä¸€ä¸ªä¸ª`tp.Dependency`æ‰€è®°å½•å‡½æ•°é—´çš„è§¦å‘å…³ç³»æ‰€æè¿°çš„ã€‚
æ‰€ä»¥è¾“å…¥çš„`pruning_fn`è¦æƒ³èƒ½è§¦å‘ä¾èµ–å…³ç³»ä¸Šçš„ç›¸å…³è£å‰ªå‡½æ•°ï¼Œå…¶å¿…é¡»æ˜¯`tp.Dependency`ä¸­å·²ç»è®°å½•è¿‡çš„å‡½æ•°ï¼Œæ¢è¨€ä¹‹ï¼Œå®ƒå¿…é¡»è¦æ˜¯ä¸€ä¸ªç»§æ‰¿äº`tp.pruner.BasePruningFunc`å­ç±»çš„ç±»æ–¹æ³•ï¼Œå…¶é€»è¾‘ä¸Šçš„åŠŸèƒ½å…¶å®æ˜¯æŒ‡å®šæ˜¯è£è¿™ä¸€å±‚çš„è¾“å…¥è¿˜æ˜¯è¾“å‡ºã€‚

å…¶è¾“å‡ºçš„ä¿®æ”¹æ–¹æ¡ˆç±»åä¸º`tp.DependencyGroup`ï¼Œå…¶æ˜¯ç”±`tp.Dependency`ç»„æˆçš„æ•°ç»„ã€‚
`DG.get_pruning_group`çš„æœ¬è´¨å°±æ˜¯ä»`module`å¯¹åº”çš„`node.dependencies`ï¼ˆ`DG.build_dependency`ä¸­æ„å»ºçš„ä¾èµ–å…³ç³»ï¼‰ä¸­æ‰¾å‡º`trigger=pruning_fn`çš„é‚£äº›`tp.Dependency`ç»„æˆ`tp.DependencyGroup`ã€‚

### `pruning_group.exec()`

æœ€åå°±æ˜¯æ‰§è¡Œè¿™ä¸ªä¿®æ”¹`DG.get_pruning_group`è¾“å‡ºçš„ä¿®æ”¹æ–¹æ¡ˆï¼Œå¾ˆå¥½ç†è§£ï¼Œå°±æ˜¯æŒ‰ç…§`tp.DependencyGroup`é‡Œçš„`tp.Dependency`ä¸€ä¸ªä¸ªæ‰§è¡Œå®ƒä»¬çš„`handler`å°±è¡Œäº†ã€‚
