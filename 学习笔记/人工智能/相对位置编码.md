# 相对位置编码

原论文：

P. Shaw, J. Uszkoreit, and A. Vaswani, ‘Self-Attention with Relative Position Representations’, in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), New Orleans, Louisiana, 2018, pp. 464–468. doi: 10.18653/v1/N18-2074.

## 没有位置编码的Attention

设$d_z$表示输入和输出特征的维度，$i\in[1,n]$，$n$表示输入样本数量。输入向量组成矩阵$X\in\mathbb R^{n\times d_z}$。

那么输出矩阵$Z\in\mathbb R^{n\times d_z}$计算过程为：

$$
Z=softmax(\frac{(X\cdot W^Q)(X\cdot W^K)^T}{\sqrt{d_z}})(X\cdot W^V)
$$

其中$W^Q\in\mathbb R^{d_{model}\times d_z}$、$W^K\in\mathbb R^{d_{model}\times d_z}$、$W^V\in\mathbb R^{d_{model}\times d_z}$

为了解释相对位置编码，将计算过程展开，输入向量$\bm x_i\in\mathbb R^{d_z}$：

$$\begin{aligned}
Z
&=softmax\left(\frac{(\left[\begin{gathered}x_1\\\vdots\\x_n\end{gathered}\right]\cdot W^Q)(\left[\begin{gathered}x_1\\\vdots\\x_n\end{gathered}\right]\cdot W^K)^T}{\sqrt{d_z}}\right)\left(\left[\begin{gathered}x_1\\\vdots\\x_n\end{gathered}\right]\cdot W^V\right)\\
&=softmax\left(\frac{\left[\begin{gathered}x_1\cdot W^Q\\\vdots\\x_n\cdot W^Q\end{gathered}\right]\left[\begin{gathered}x_1\cdot W^K\\\vdots\\x_n\cdot W^K\end{gathered}\right]^T}{\sqrt{d_z}}\right)\left[\begin{gathered}x_1\cdot W^V\\\vdots\\x_n\cdot W^V\end{gathered}\right]\\
&=softmax\left(\frac{(\left[\begin{gathered}x_1\cdot W^Q\\\vdots\\x_n\cdot W^Q\end{gathered}\right])\left[\begin{gathered}(x_1\cdot W^K)^T,\dots,(x_n\cdot W^K)^T\end{gathered}\right]}{\sqrt{d_z}}\right)\left[\begin{gathered}x_1\cdot W^V\\\vdots\\x_n\cdot W^V\end{gathered}\right]\\
&=softmax\left(\frac{\left\{(x_i\cdot W^Q)(x_j\cdot W^K)^T\right\}_{i,j\in[1,n]}}{\sqrt{d_z}}\right)\left[\begin{gathered}x_1\cdot W^V\\\vdots\\x_n\cdot W^V\end{gathered}\right]\\
&=\left\{softmax\left(\frac{(x_i\cdot W^Q)(x_j\cdot W^K)^T}{\sqrt{d_z}}\right)\right\}_{i,j\in[1,n]}\left[\begin{gathered}x_1\cdot W^V\\\vdots\\x_n\cdot W^V\end{gathered}\right]\\
&=\left[\sum_{j=1}^nsoftmax\left(\frac{(x_i\cdot W^Q)(x_j\cdot W^K)^T}{\sqrt{d_z}}\right)(x_j\cdot W^V)\right]_{i\in[1,n]}\\
&=\left[\begin{gathered}z_1\\\vdots\\z_n\end{gathered}\right]
\end{aligned}$$

所以：
$$
z_i=\sum_{j=1}^nsoftmax\left(\frac{(x_i\cdot W^Q)(x_j\cdot W^K)^T}{\sqrt{d_z}}\right)(x_j\cdot W^V)
$$

## 加上相对位置编码的Attention

$$
z_i=\sum_{j=1}^nsoftmax\left(\frac{(x_i\cdot W^Q)(x_j\cdot W^K+a_{ij}^K)^T}{\sqrt{d_z}}\right)(x_j\cdot W^V+a_{ij}^V)
$$

$$
\begin{aligned}
a_{ij}^K&=w_{clip}^K(j-i,k)\\
a_{ij}^V&=w_{clip}^V(j-i,k)\\
clip(x,k)&=max(-k,min(k,x))
\end{aligned}
$$
