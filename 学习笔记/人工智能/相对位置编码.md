# ç›¸å¯¹ä½ç½®ç¼–ç 

åŸè®ºæ–‡ï¼š

P. Shaw, J. Uszkoreit, and A. Vaswani, â€˜Self-Attention with Relative Position Representationsâ€™, in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), New Orleans, Louisiana, 2018, pp. 464â€“468. doi: 10.18653/v1/N18-2074.

## æ²¡æœ‰ä½ç½®ç¼–ç çš„Attention

è®¾$d_z$è¡¨ç¤ºè¾“å…¥å’Œè¾“å‡ºç‰¹å¾çš„ç»´åº¦ï¼Œ$i\in[1,n]$ï¼Œ$n$è¡¨ç¤ºè¾“å…¥æ ·æœ¬æ•°é‡ã€‚è¾“å…¥å‘é‡ç»„æˆçŸ©é˜µ$X\in\mathbb R^{n\times d_z}$ã€‚

é‚£ä¹ˆè¾“å‡ºçŸ©é˜µ$Z\in\mathbb R^{n\times d_z}$è®¡ç®—è¿‡ç¨‹ä¸ºï¼š

$$
Z=softmax(\frac{(X W^Q)(X W^K)^T}{\sqrt{d_z}})(X W^V)
$$

å…¶ä¸­$W^Q\in\mathbb R^{d_{model}\times d_z}$ã€$W^K\in\mathbb R^{d_{model}\times d_z}$ã€$W^V\in\mathbb R^{d_{model}\times d_z}$

ä¸ºäº†è§£é‡Šç›¸å¯¹ä½ç½®ç¼–ç ï¼Œå°†è®¡ç®—è¿‡ç¨‹å±•å¼€ï¼Œè¾“å…¥å‘é‡$\bm x_i\in\mathbb R^{d_z}$ï¼š

$$\begin{aligned}
Z
&=softmax\left(\frac{\left[\begin{gathered}x_1\\\vdots\\x_n\end{gathered}\right]\cdot W^Q\cdot (\left[\begin{gathered}x_1\\\vdots\\x_n\end{gathered}\right]\cdot W^K)^T}{\sqrt{d_z}}\right)\cdot \left[\begin{gathered}x_1\\\vdots\\x_n\end{gathered}\right]\cdot W^V\\
&=softmax\left(\frac{\left[\begin{gathered}x_1 W^Q\\\vdots\\x_n W^Q\end{gathered}\right]\left[\begin{gathered}x_1 W^K\\\vdots\\x_n W^K\end{gathered}\right]^T}{\sqrt{d_z}}\right)\left[\begin{gathered}x_1 W^V\\\vdots\\x_n W^V\end{gathered}\right]\\
&=softmax\left(\frac{(\left[\begin{gathered}x_1 W^Q\\\vdots\\x_iW^Q\\\vdots\\x_n W^Q\end{gathered}\right])\left[\begin{gathered}(x_1 W^K)^T,\dots,(x_jW^K)^T\dots,(x_n W^K)^T\end{gathered}\right]}{\sqrt{d_z}}\right)\left[\begin{gathered}x_1 W^V\\\vdots\\x_jW^V\\\vdots\\x_n W^V\end{gathered}\right]\\
&=softmax\left(\frac{\left\{x_i W^Q(x_j W^K)^T\right\}_{i,j\in[1,n]}}{\sqrt{d_z}}\right)\left[\begin{gathered}x_1 W^V\\\vdots\\x_jW^V\\\vdots\\x_n W^V\end{gathered}\right]\\
&=\left\{softmax\left(\frac{x_i W^Q(x_j W^K)^T}{\sqrt{d_z}}\right)\right\}_{i,j\in[1,n]}\left[\begin{gathered}x_1 W^V\\\vdots\\x_jW^V\\\vdots\\x_n W^V\end{gathered}\right]\\
&=\left[\sum_{j=1}^nsoftmax\left(\frac{x_i W^Q(x_j W^K)^T}{\sqrt{d_z}}\right)x_j W^V\right]_{i\in[1,n]}\\
&=\left[\begin{gathered}z_1\\\vdots\\z_n\end{gathered}\right]
\end{aligned}$$

æ‰€ä»¥ï¼š
$$
z_i=\sum_{j=1}^nsoftmax\left(\frac{x_i W^Q(x_j W^K)^T}{\sqrt{d_z}}\right)x_j W^V
$$

## åŠ ä¸Šç›¸å¯¹ä½ç½®ç¼–ç çš„Attention

$$
z_i=\sum_{j=1}^nsoftmax\left(\frac{x_i W^Q(x_j W^K+a_{ij}^K)^T}{\sqrt{d_z}}\right)(x_j W^V+a_{ij}^V)
$$

å…¶ä¸­ï¼Œ$a_{ij}^K$å’Œ$a_{ij}^V$å°±æ˜¯åºåˆ—ä¸­çš„å…ƒç´ $i$ç›¸å¯¹äºå…ƒç´ $j$çš„ç›¸å¯¹ä½ç½®ç¼–ç ã€‚è¿›ä¸€æ­¥ï¼Œè¿™ä¸¤é¡¹ä½ç½®ç¼–ç å€¼éƒ½å–è‡ªé•¿ä¸º$2k+1$çš„ä½ç½®ç¼–ç é›†ï¼š

$$
\begin{aligned}
w^K&=(w_{-k}^K,\dots,w_k^K)\\
w^V&=(w_{-k}^V,\dots,w_k^V)
\end{aligned}
$$

å…¶å–æ³•å¦‚ä¸‹ï¼š

$$
\begin{aligned}
a_{ij}^K&=w_{clip}^K(j-i,k)\\
a_{ij}^V&=w_{clip}^V(j-i,k)\\
clip(x,k)&=max(-k,min(k,x))
\end{aligned}
$$

å…¶å®å°±æ˜¯æ ¹æ®$ij$å·®å€¼å»$w^K$å’Œ$w^V$é‡Œå–å€¼ï¼Œå¹¶ä¸”è®¾å®šå·®å€¼æœ€å¤§ä¸º$k$ï¼Œè¶…è¿‡$k$çš„ä½ç½®ç¼–ç ä¸ºå›ºå®šå€¼ã€‚

ä¹ŸæŒºå¥½ç†è§£çš„ã€‚

## ç›¸å¯¹ä½ç½®ç¼–ç çš„çŸ©é˜µå½¢å¼

$$\begin{aligned}
Z
&=\left[\begin{gathered}z_1\\\vdots\\z_n\end{gathered}\right]\\
&=\left[\sum_{j=1}^nsoftmax\left(\frac{x_i W^Q(x_j W^K+a_{ij}^K)^T}{\sqrt{d_z}}\right)(x_j W^V+a_{ij}^V)\right]_{i\in[1,n]}\\
&=\left[\sum_{j=1}^nsoftmax\left(\frac{x_i W^Q(x_j W^K)^T+x_i W^Q(a_{ij}^K)^T}{\sqrt{d_z}}\right)(x_j W^V+a_{ij}^V)\right]_{i\in[1,n]}
\end{aligned}$$

è¿™ä¹ˆä¸€çœ‹$a_{ij}^K$å’Œ$a_{ij}^V$éƒ½æ˜¯å‘é‡ï¼Œé‚£ç”±$a_{ij}^K$å’Œ$a_{ij}^V$ç»„æˆçš„çŸ©é˜µå°±æ˜¯ä¸ª$\mathbb R^{n\times n\times d_z}$çš„ä¸‰ç»´çŸ©é˜µäº†ï¼Œæ²¡æ³•è¡¨ç¤ºå•ŠğŸ˜‚ï¼Œè¿˜æ˜¯ç®—äº†å§

## åœ¨ä»£ç ä¸­çš„ä½“ç°

æ¥ä¸€æ®µå›¾åƒRSTTçš„Pythonä»£ç ã€‚RSTTæ˜¯ä¸€ç§èƒ½åŒæ—¶è¿›è¡Œæ’å¸§å’Œè¶…åˆ†è¾¨ç‡çš„Transformerï¼š

```python
class WindowAttention3D(nn.Module):
    """Window based multi-head self/cross attention (W-MSA/W-MCA) module with relative 
    position bias. 
    It supports both of shifted and non-shifted window.
    """
    def __init__(self, dim, num_frames_q, num_frames_kv, window_size, num_heads,
                 qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):
        """Initialization function.

        Args:
            dim (int): Number of input channels.
            num_frames (int): Number of input frames.
            window_size (tuple[int]): The size of the window.
            num_heads (int): Number of attention heads.
            qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Defaults to True.
            qk_scale (float, optional): Override default qk scale of head_dim ** -0.5 if set. Defaults to None.
            attn_drop (float, optional): Dropout ratio of attention weight. Defaults to 0.0
            proj_drop (float, optional): Dropout ratio of output. Defaults to 0.0
        """
        super().__init__()
        self.dim = dim
        self.num_frames_q = num_frames_q # D1
        self.num_frames_kv = num_frames_kv # D2
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads # nH
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # è¿™ä¸ªrelative_position_bias_tableä¸€çœ‹å°±æ˜¯ä½ç½®ç¼–ç 
        # åå­—å–å¾—å«ä½ç½®ç¼–ç ä¸è¯´ï¼Œè¿˜æœ‰å¾ˆæ˜æ˜¾çš„2n-1è®¡ç®—ï¼Œå¾ˆæ˜¾ç„¶æ˜¯w
        self.relative_position_bias_table = nn.Parameter( # TODO
            torch.zeros((2 * num_frames_q - 1) * (2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)
        )  # 2*D-1 * 2*Wh-1 * 2*Ww-1, nH
        # è¿™ä¸ªAttentionå±‚ä¸­çš„ç›¸å¯¹ä½ç½®ç¼–ç æ˜¯æŒ‰çª—å£å¤§å°å®šçš„
        # ä»”ç»†çœ‹çœ‹è¿™é‡Œçš„2n-1è®¡ç®—æ˜¯å¯¹äºnum_frames_qå’Œwindow_sizeè¿›è¡Œçš„ï¼Œå¹¶ä¸”è¿˜æŠŠå®ƒä»¬ä¹˜èµ·æ¥äº†ã€‚
        # é‚£ä¹ˆå¯ä»¥æ¨æ–­è¿™ä¸ªä½ç½®ç¼–ç åŒ…å«å¤šä¸ªç»´åº¦ï¼šç‰¹å¾åœ¨ä¸åŒå¸§ä¸Šçš„ç›¸å¯¹ä½ç½®ã€ç‰¹å¾åœ¨å›¾ç‰‡ä¸Šçš„ç›¸å¯¹ä½ç½®ï¼ˆé•¿å®½ä¸¤ä¸ªç»´åº¦ï¼‰

        # Get pair-wise relative position index for each token inside the window
        coords_d_q = torch.arange(self.num_frames_q) # ä»1æ•°åˆ°Qçš„å¸§æ•°
        coords_d_kv = torch.arange(0, self.num_frames_q, int((self.num_frames_q + 1) // self.num_frames_kv)) # ä»1æ•°åˆ°KVçš„å¸§æ•°
        # æ³¨æ„ï¼šåœ¨RSTTä¸­ï¼Œnum_frames_qç­‰äºè¾“å‡ºå¸§çš„æ•°é‡ï¼Œå³è¾“å…¥æ ·æœ¬è¿›è¡Œæ’å¸§åçš„å¸§æ•°ï¼›
        # è€Œnum_frames_kvæ¥è‡ªäºæœªæ’å¸§çš„åŸå§‹æ•°æ®ï¼Œæ‰€ä»¥è¿™é‡Œçš„coords_d_kvä»1æ•°åˆ°KVçš„å¸§æ•°æ˜¯è·³ç€æ•°çš„

        coords_h = torch.arange(self.window_size[0]) # ä»1æ•°åˆ°çª—å£é•¿åº¦
        coords_w = torch.arange(self.window_size[1]) # ä»1æ•°åˆ°çª—å£å®½åº¦

        # æ¥ä¸‹æ¥meshgridæŠŠä¸Šé¢çš„è¿™å‡ ä¸ªæ•°æ•°çš„æ•°ç»„ç»„æˆåæ ‡è¡¨
        coords_q = torch.stack(torch.meshgrid([coords_d_q, coords_h, coords_w]))  # 3, D1, Wh, Ww
        coords_kv = torch.stack(torch.meshgrid([coords_d_kv, coords_h, coords_w]))  # 3, D2, Wh, Ww

        # ç„¶åæ‹‰å¹³
        coords_q_flatten = torch.flatten(coords_q, 1)  # 3, D1*Wh*Ww
        coords_kv_flatten = torch.flatten(coords_kv, 1)  # 3, D2*Wh*Ww

        # æ¥ä¸‹æ¥å°±æ˜¯è®¡ç®—ç›¸å¯¹é‡ï¼Œå°±æ˜¯åœ¨wé‡Œå–æ•°æ—¶ç”¨çš„kå€¼
        relative_coords = coords_q_flatten[:, :, None] - coords_kv_flatten[:, None, :]  # 3, D1*Wh*Ww, D2*Wh*Ww
        # è¿™ä¸ªNoneä¼¼ä¹å¯ä»¥è®©çŸ©é˜µåœ¨è¿ç®—æ—¶æ‰©å±•ï¼Œäºæ˜¯è®©(3, D1*Wh*Ww, None)ä¸(3, None, D2*Wh*Ww)çš„è®¡ç®—ç»“æœå˜æˆ(3, D1*Wh*Ww, D2*Wh*Ww)
        # äºæ˜¯ç°åœ¨relative_coordså°±æ˜¯è¾“å…¥ä¸­æ¯ä¸ªå€¼ä¸å…¶ä»–æ‰€æœ‰å€¼çš„indexä¹‹å·®äº†
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # D1*Wh*Ww, D2*Wh*Ww, 3
        # ç°åœ¨relative_coordsé‡Œé¢æœ‰è´Ÿå€¼ï¼Œç„¶åä¸‹é¢è¿™ä¸€æ­¥å°±æ˜¯è®©æ‰€æœ‰çš„indexéƒ½ä»0å¼€å§‹
        relative_coords[:, :, 0] += self.num_frames_q - 1
        relative_coords[:, :, 1] += self.window_size[0] - 1
        relative_coords[:, :, 2] += self.window_size[1] - 1

        # ç°åœ¨relative_coordsé‡Œé¢å°±æ˜¯æ¯ä¸ªç»´åº¦çš„è·ç¦»ï¼Œç„¶åæ¥ä¸‹æ¥å°±æ˜¯è®¡ç®—ç´¯è®¡è·ç¦»
        relative_coords[:, :, 0] *= (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1) # çœ‹æ¥æ¯ä¸€å¸§æ˜¯æœ€å¤§çš„è·ç¦»å•ä½
        relative_coords[:, :, 1] *= 2 * self.window_size[1] - 1 # window_size[1]æ˜¯ç¬¬äºŒå¤§çš„è·ç¦»å•ä½
        # æ²¡æœ‰relative_coords[:, :, 2]ï¼Ÿå› ä¸ºself.window_size[0]æ˜¯æœ€å°çš„è·ç¦»å•ä½å•¦

        # æ±‚å’Œï¼Œå¾—åˆ°æœ€ç»ˆçš„kå€¼
        relative_position_index = relative_coords.sum(-1)  # D1*Wh*Ww, D2*Wh*Ww
        self.register_buffer("relative_position_index", relative_position_index)
        # ç›¸å¯¹ä½ç½®ç¼–ç åˆå§‹åŒ–å®Œæˆ

        self.q = nn.Linear(dim, dim, bias=qkv_bias)
        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, q, kv=None, mask=None):
        """Forward function.

        Args:
            q (torch.Tensor): (B*nW, D1*Wh*Ww, C)
            kv (torch.Tensor): (B*nW, D2*Wh*Ww, C). Defaults to None.
            mask (torch.Tensor, optional): Mask for shifted window attention (nW, D1*Wh*Ww, D2*Wh*Ww). Defaults to None.

        Returns:
            torch.Tensor: (B*nW, D1*Wh*Ww, C)
        """
        kv = q if kv is None else kv
        B_, N1, C = q.shape # N1 = D1*Wh*Ww, B_ = B*nW
        B_, N2, C = kv.shape # N2 = D2*Wh*Ww, B_ = B*nW

        q = self.q(q).reshape(B_, N1, 1, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        kv = self.kv(kv).reshape(B_, N2, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = q[0], kv[0], kv[1] # B_, nH, N1(2), C
        q = q * self.scale
        attn = (q @ k.transpose(-2, -1)) # B_, nH, N1, N2

        # ç›¸å¯¹ä½ç½®ç¼–ç ç”¨æ³•å°±éå¸¸ç®€å•äº†ï¼Œç›´æ¥æŒ‰kå€¼è¿›wé‡Œå–æ•°ç„¶åä¸attantionç›¸åŠ å°±å®Œäº†
        # è¿™ä¸€æ­¥æ˜¯å–æ•°
        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
            N1, N2, -1)  # D1*Wh*Ww, D2*Wh*Ww, nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, D1*Wh*Ww, D2*Wh*Ww
        # è¿™ä¸€æ­¥æ˜¯ç›¸åŠ 
        attn = attn + relative_position_bias.unsqueeze(0) # B_, nH, D1*Wh*Ww, D2*Wh*Ww

        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N1, N2) + mask.unsqueeze(1).unsqueeze(0) # B, nW, nH, D1*Wh*Ww, D2*Wh*Ww
            attn = attn.view(-1, self.num_heads, N1, N2)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)

        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B_, N1, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x, attn
```
