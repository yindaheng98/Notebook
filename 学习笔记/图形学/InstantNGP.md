# Instant-NGPä¸­çš„NeRFæ¸²æŸ“æ ¸å¿ƒä»£ç è§£æ

Instant-NGPä»£ç å¾ˆå¤šï¼Œä½†æ˜¯å¤§éƒ¨åˆ†éƒ½æ˜¯ç»™äº¤äº’åŠŸèƒ½å†™çš„ä»£ç ï¼ŒçœŸæ­£çš„è®­ç»ƒå’Œæ¸²æŸ“çš„ä»£ç åªå æ‰€æœ‰ä»£ç çš„ä¸€å°éƒ¨åˆ†ã€‚

Instant-NGPä¸­çš„NeRFæ¸²æŸ“ä¸»å‡½æ•°æ˜¯`render_nerf`ï¼Œè°ƒç”¨æ ˆä½äº`frame->train_and_render->render_frame->render_frame_main->render_nerf`ã€‚
å…¶ç»“æ„å¦‚ä¸‹ï¼š

```cpp
void Testbed::render_nerf(
	cudaStream_t stream,
	CudaDevice& device,
	const CudaRenderBufferView& render_buffer,
	const std::shared_ptr<NerfNetwork<network_precision_t>>& nerf_network,
	const uint8_t* density_grid_bitfield,
	const vec2& focal_length,
	const mat4x3& camera_matrix0,
	const mat4x3& camera_matrix1,
	const vec4& rolling_shutter,
	const vec2& screen_center,
	const Foveation& foveation,
	int visualized_dimension
) {
	float plane_z = m_slice_plane_z + m_scale;
	if (m_render_mode == ERenderMode::Slice) {
		plane_z = -plane_z;
	}

	ERenderMode render_mode = visualized_dimension > -1 ? ERenderMode::EncodingVis : m_render_mode;

	const float* extra_dims_gpu = m_nerf.get_rendering_extra_dims(stream);

	NerfTracer tracer;

	// Our motion vector code can't undo grid distortions -- so don't render grid distortion if DLSS is enabled.
	// (Unless we're in distortion visualization mode, in which case the distortion grid is fine to visualize.)
	auto grid_distortion =
		m_nerf.render_with_lens_distortion && (!m_dlss || m_render_mode == ERenderMode::Distortion) ?
		m_distortion.inference_view() :
		Buffer2DView<const vec2>{};

	Lens lens = m_nerf.render_with_lens_distortion ? m_nerf.render_lens : Lens{};

	auto resolution = render_buffer.resolution;

	tracer.init_rays_from_camera(
		render_buffer.spp,
		nerf_network->padded_output_width(),
		nerf_network->n_extra_dims(),
		render_buffer.resolution,
		focal_length,
		camera_matrix0,
		camera_matrix1,
		rolling_shutter,
		screen_center,
		m_parallax_shift,
		m_snap_to_pixel_centers,
		m_render_aabb,
		m_render_aabb_to_local,
		m_render_near_distance,
		plane_z,
		m_aperture_size,
		foveation,
		lens,
		m_envmap.inference_view(),
		grid_distortion,
		render_buffer.frame_buffer,
		render_buffer.depth_buffer,
		render_buffer.hidden_area_mask ? render_buffer.hidden_area_mask->const_view() : Buffer2DView<const uint8_t>{},
		density_grid_bitfield,
		m_nerf.show_accel,
		m_nerf.max_cascade,
		m_nerf.cone_angle_constant,
		render_mode,
		stream
	);

	float depth_scale = 1.0f / m_nerf.training.dataset.scale;
	bool render_2d = m_render_mode == ERenderMode::Slice || m_render_mode == ERenderMode::Distortion;

	uint32_t n_hit;
	if (render_2d) {
		n_hit = tracer.n_rays_initialized();
	} else {
		n_hit = tracer.trace(
			nerf_network,
			m_render_aabb,
			m_render_aabb_to_local,
			m_aabb,
			focal_length,
			m_nerf.cone_angle_constant,
			density_grid_bitfield,
			render_mode,
			camera_matrix1,
			depth_scale,
			m_visualized_layer,
			visualized_dimension,
			m_nerf.rgb_activation,
			m_nerf.density_activation,
			m_nerf.show_accel,
			m_nerf.max_cascade,
			m_nerf.render_min_transmittance,
			m_nerf.glow_y_cutoff,
			m_nerf.glow_mode,
			extra_dims_gpu,
			stream
		);
	}
	RaysNerfSoa& rays_hit = render_2d ? tracer.rays_init() : tracer.rays_hit();

	if (render_2d) {
		// Store colors in the normal buffer
		uint32_t n_elements = next_multiple(n_hit, BATCH_SIZE_GRANULARITY);
		const uint32_t floats_per_coord = sizeof(NerfCoordinate) / sizeof(float) + nerf_network->n_extra_dims();
		const uint32_t extra_stride = nerf_network->n_extra_dims() * sizeof(float); // extra stride on top of base NerfCoordinate struct

		GPUMatrix<float> positions_matrix{floats_per_coord, n_elements, stream};
		GPUMatrix<float> rgbsigma_matrix{4, n_elements, stream};

		linear_kernel(generate_nerf_network_inputs_at_current_position, 0, stream, n_hit, m_aabb, rays_hit.payload, PitchedPtr<NerfCoordinate>((NerfCoordinate*)positions_matrix.data(), 1, 0, extra_stride), extra_dims_gpu);

		if (visualized_dimension == -1) {
			nerf_network->inference(stream, positions_matrix, rgbsigma_matrix);
			linear_kernel(compute_nerf_rgba_kernel, 0, stream, n_hit, (vec4*)rgbsigma_matrix.data(), m_nerf.rgb_activation, m_nerf.density_activation, 0.01f, false);
		} else {
			nerf_network->visualize_activation(stream, m_visualized_layer, visualized_dimension, positions_matrix, rgbsigma_matrix);
		}

		linear_kernel(shade_kernel_nerf, 0, stream,
			n_hit,
			m_nerf.render_gbuffer_hard_edges,
			camera_matrix1,
			depth_scale,
			(vec4*)rgbsigma_matrix.data(),
			nullptr,
			rays_hit.payload,
			m_render_mode,
			m_nerf.training.linear_colors,
			render_buffer.frame_buffer,
			render_buffer.depth_buffer
		);
		return;
	}

	linear_kernel(shade_kernel_nerf, 0, stream,
		n_hit,
		m_nerf.render_gbuffer_hard_edges,
		camera_matrix1,
		depth_scale,
		rays_hit.rgba,
		rays_hit.depth,
		rays_hit.payload,
		m_render_mode,
		m_nerf.training.linear_colors,
		render_buffer.frame_buffer,
		render_buffer.depth_buffer
	);

	if (render_mode == ERenderMode::Cost) {
		std::vector<NerfPayload> payloads_final_cpu(n_hit);
		CUDA_CHECK_THROW(cudaMemcpyAsync(payloads_final_cpu.data(), rays_hit.payload, n_hit * sizeof(NerfPayload), cudaMemcpyDeviceToHost, stream));
		CUDA_CHECK_THROW(cudaStreamSynchronize(stream));

		size_t total_n_steps = 0;
		for (uint32_t i = 0; i < n_hit; ++i) {
			total_n_steps += payloads_final_cpu[i].n_steps;
		}
		tlog::info() << "Total steps per hit= " << total_n_steps << "/" << n_hit << " = " << ((float)total_n_steps/(float)n_hit);
	}
}
```

å¯ä»¥çœ‹åˆ°ï¼Œä¸»è¦æµç¨‹æœ‰ä¸‰ï¼š
* `tracer.init_rays_from_camera`
* `tracer.trace`
* `shade_kernel_nerf`

## ç”Ÿæˆå…‰çº¿`tracer.init_rays_from_camera`

é¡¾åæ€ä¹‰ï¼Œå¯¹è¦æ¸²æŸ“çš„å›¾åƒä¸Šçš„æ¯ä¸ªåƒç´ ç”Ÿæˆä¸€é“å…‰çº¿ï¼Œç”¨äºray marchingã€‚

```cpp
void Testbed::NerfTracer::init_rays_from_camera(
	uint32_t sample_index,
	uint32_t padded_output_width,
	uint32_t n_extra_dims,
	const ivec2& resolution,
	const vec2& focal_length,
	const mat4x3& camera_matrix0,
	const mat4x3& camera_matrix1,
	const vec4& rolling_shutter,
	const vec2& screen_center,
	const vec3& parallax_shift,
	bool snap_to_pixel_centers,
	const BoundingBox& render_aabb,
	const mat3& render_aabb_to_local,
	float near_distance,
	float plane_z,
	float aperture_size,
	const Foveation& foveation,
	const Lens& lens,
	const Buffer2DView<const vec4>& envmap,
	const Buffer2DView<const vec2>& distortion,
	vec4* frame_buffer,
	float* depth_buffer,
	const Buffer2DView<const uint8_t>& hidden_area_mask,
	const uint8_t* grid,
	int show_accel,
	uint32_t max_mip,
	float cone_angle_constant,
	ERenderMode render_mode,
	cudaStream_t stream
) {
	// Make sure we have enough memory reserved to render at the requested resolution
	size_t n_pixels = (size_t)resolution.x * resolution.y;
	enlarge(n_pixels, padded_output_width, n_extra_dims, stream);

	const dim3 threads = { 16, 8, 1 };
	const dim3 blocks = { div_round_up((uint32_t)resolution.x, threads.x), div_round_up((uint32_t)resolution.y, threads.y), 1 };
	init_rays_with_payload_kernel_nerf<<<blocks, threads, 0, stream>>>(
		sample_index,
		m_rays[0].payload,
		resolution,
		focal_length,
		camera_matrix0,
		camera_matrix1,
		rolling_shutter,
		screen_center,
		parallax_shift,
		snap_to_pixel_centers,
		render_aabb,
		render_aabb_to_local,
		near_distance,
		plane_z,
		aperture_size,
		foveation,
		lens,
		envmap,
		frame_buffer,
		depth_buffer,
		hidden_area_mask,
		distortion,
		render_mode
	);

	m_n_rays_initialized = resolution.x * resolution.y;

	CUDA_CHECK_THROW(cudaMemsetAsync(m_rays[0].rgba, 0, m_n_rays_initialized * sizeof(vec4), stream));
	CUDA_CHECK_THROW(cudaMemsetAsync(m_rays[0].depth, 0, m_n_rays_initialized * sizeof(float), stream));

	linear_kernel(advance_pos_nerf_kernel, 0, stream,
		m_n_rays_initialized,
		render_aabb,
		render_aabb_to_local,
		camera_matrix1[2],
		focal_length,
		sample_index,
		m_rays[0].payload,
		grid,
		(show_accel >= 0) ? show_accel : 0,
		max_mip,
		cone_angle_constant
	);
}
```

å¯ä»¥çœ‹åˆ°ï¼Œä¸»è¦æµç¨‹æœ‰äºŒï¼š
* `init_rays_with_payload_kernel_nerf`ï¼šåˆå§‹åŒ–`payload`ï¼Œè®¡ç®—å…‰çº¿çš„èµ·ç‚¹å’Œæ–¹å‘ï¼Œä¿å­˜åœ¨`payload`ä¸­
* `advance_pos_nerf_kernel`ï¼šæ ¹æ®`payload`ä¸­å…‰çº¿çš„èµ·ç‚¹å’Œæ–¹å‘å’Œ`density_grid`è®¡ç®—é‡‡æ ·çš„èŒƒå›´

### åˆå§‹åŒ–å…‰çº¿ï¼š`init_rays_with_payload_kernel_nerf`

```cpp
__global__ void init_rays_with_payload_kernel_nerf(
	uint32_t sample_index,
	NerfPayload* __restrict__ payloads,
	ivec2 resolution,
	vec2 focal_length,
	mat4x3 camera_matrix0,
	mat4x3 camera_matrix1,
	vec4 rolling_shutter,
	vec2 screen_center,
	vec3 parallax_shift,
	bool snap_to_pixel_centers,
	BoundingBox render_aabb,
	mat3 render_aabb_to_local,
	float near_distance,
	float plane_z,
	float aperture_size,
	Foveation foveation,
	Lens lens,
	Buffer2DView<const vec4> envmap,
	vec4* __restrict__ frame_buffer,
	float* __restrict__ depth_buffer,
	Buffer2DView<const uint8_t> hidden_area_mask,
	Buffer2DView<const vec2> distortion,
	ERenderMode render_mode
) {
	uint32_t x = threadIdx.x + blockDim.x * blockIdx.x;
	uint32_t y = threadIdx.y + blockDim.y * blockIdx.y;

	if (x >= resolution.x || y >= resolution.y) {
		return;
	}

	uint32_t idx = x + resolution.x * y;

	if (plane_z < 0) {
		aperture_size = 0.0;
	}

	vec2 pixel_offset = ld_random_pixel_offset(snap_to_pixel_centers ? 0 : sample_index);
	vec2 uv = vec2{(float)x + pixel_offset.x, (float)y + pixel_offset.y} / vec2(resolution);
	mat4x3 camera = get_xform_given_rolling_shutter({camera_matrix0, camera_matrix1}, rolling_shutter, uv, ld_random_val(sample_index, idx * 72239731));

	Ray ray = uv_to_ray(
		sample_index,
		uv,
		resolution,
		focal_length,
		camera,
		screen_center,
		parallax_shift,
		near_distance,
		plane_z,
		aperture_size,
		foveation,
		hidden_area_mask,
		lens,
		distortion
	);

	NerfPayload& payload = payloads[idx];
	payload.max_weight = 0.0f;

	depth_buffer[idx] = MAX_DEPTH();

	if (!ray.is_valid()) {
		payload.origin = ray.o;
		payload.alive = false;
		return;
	}

	if (plane_z < 0) {
		float n = length(ray.d);
		payload.origin = ray.o;
		payload.dir = (1.0f/n) * ray.d;
		payload.t = -plane_z*n;
		payload.idx = idx;
		payload.n_steps = 0;
		payload.alive = false;
		depth_buffer[idx] = -plane_z;
		return;
	}

	if (render_mode == ERenderMode::Distortion) {
		vec2 uv_after_distortion = pos_to_uv(ray(1.0f), resolution, focal_length, camera, screen_center, parallax_shift, foveation);

		frame_buffer[idx].rgb() = to_rgb((uv_after_distortion - uv) * 64.0f);
		frame_buffer[idx].a = 1.0f;
		depth_buffer[idx] = 1.0f;
		payload.origin = ray(MAX_DEPTH());
		payload.alive = false;
		return;
	}

	ray.d = normalize(ray.d);

	if (envmap) {
		frame_buffer[idx] = read_envmap(envmap, ray.d);
	}

	float t = fmaxf(render_aabb.ray_intersect(render_aabb_to_local * ray.o, render_aabb_to_local * ray.d).x, 0.0f) + 1e-6f;

	if (!render_aabb.contains(render_aabb_to_local * ray(t))) {
		payload.origin = ray.o;
		payload.alive = false;
		return;
	}

	payload.origin = ray.o;
	payload.dir = ray.d;
	payload.t = t;
	payload.idx = idx;
	payload.n_steps = 0;
	payload.alive = true;
}
```

å¯ä»¥çœ‹åˆ°ï¼Œæœ€ä¸»è¦çš„è®¡ç®—æ˜¯`uv_to_ray`è¿™ä¸ªå‡½æ•°ï¼Œå…¶è¾“å…¥å„ç§ç›¸æœºå‚æ•°å’Œåƒç´ ä½ç½®è¾“å‡ºä¸€ä¸ª`Ray`ç±»å‹çš„å˜é‡ï¼š

```cpp
struct Ray {
	vec3 o;
	vec3 d;

	NGP_HOST_DEVICE vec3 operator()(float t) const {
		return o + t * d;
	}

	NGP_HOST_DEVICE void advance(float t) {
		o += d * t;
	}

	NGP_HOST_DEVICE float distance_to(const vec3& p) const {
		vec3 nearest = p - o;
		nearest -= d * dot(nearest, d) / length2(d);
		return length(nearest);
	}

	NGP_HOST_DEVICE bool is_valid() const {
		return d != vec3(0.0f);
	}

	static NGP_HOST_DEVICE Ray invalid() {
		return {{0.0f, 0.0f, 0.0f}, {0.0f, 0.0f, 0.0f}};
	}
};
```

`o`ã€`d`ã€`t`éƒ½æ˜¯å¾ˆå¸¸è§çš„è¡¨ç¤ºå…‰çº¿èµ·ç‚¹ã€æ–¹å‘ã€å‰è¿›è·ç¦»çš„å˜é‡åç»„åˆï¼Œå¾ˆæ˜¾ç„¶`uv_to_ray`å°±æ˜¯åœ¨è®¡ç®—å…‰çº¿çš„èµ·ç‚¹å’Œæ–¹å‘ã€‚

`init_rays_with_payload_kernel_nerf`ä¸­`uv_to_ray`ä¹‹åå°±æ˜¯å„ç§èµ‹å€¼äº†ï¼Œå¯ä»¥çœ‹åˆ°æ˜¯æŠŠ`o`ã€`d`ã€`t`èµ‹å€¼ç»™`payload`ï¼Œæ¯ä¸ª`payload`å¯¹åº”å›¾åƒä¸Šçš„ä¸€ä¸ªåƒç´ ï¼Œå…¶å€¼çš„å«ä¹‰ä¹Ÿå¯ä»¥çŒœå‡ºæ¥ï¼š

```cpp
	payload.origin = ray.o; // å…‰çº¿çš„èµ·ç‚¹
	payload.dir = ray.d; // å…‰çº¿çš„æ–¹å‘
	payload.t = t; // å…‰çº¿ä¸Šå¯ä»¥é‡‡æ ·çš„æœ€è¿œè·ç¦»
	payload.idx = idx; // åƒç´ çš„IDï¼Œç”¨äºè¡¨ç¤ºè¿™ä¸ªpayloadæ˜¯å›¾åƒä¸Šä¸Šçš„å“ªä¸ªåƒç´ 
	payload.n_steps = 0; // ä¹‹åray marchingä¼šç”¨åˆ°ï¼Œç”¨äºè®°å½•é‡‡æ ·ç‚¹æ•°é‡
	payload.alive = true; // ä¹‹åray marchingä¼šç”¨åˆ°ï¼Œå…‰çº¿ä¸ä¸€å®šéƒ½ä¼šhitåˆ°åœºæ™¯ä¸­çš„ç‚¹ï¼Œhitä¸åˆ°ç‚¹çš„å…‰çº¿å°±æ˜¯payload.alive = false
```

### è®¡ç®—é‡‡æ ·èŒƒå›´ï¼š`advance_pos_nerf_kernel`

```cpp
__global__ void advance_pos_nerf_kernel(
	const uint32_t n_elements,
	BoundingBox render_aabb,
	mat3 render_aabb_to_local,
	vec3 camera_fwd,
	vec2 focal_length,
	uint32_t sample_index,
	NerfPayload* __restrict__ payloads,
	const uint8_t* __restrict__ density_grid,
	uint32_t min_mip,
	uint32_t max_mip,
	float cone_angle_constant
) {
	const uint32_t i = threadIdx.x + blockIdx.x * blockDim.x;
	if (i >= n_elements) return;

	advance_pos_nerf(payloads[i], render_aabb, render_aabb_to_local, camera_fwd, focal_length, sample_index, density_grid, min_mip, max_mip, cone_angle_constant);
}
```

æ²¡ä»€ä¹ˆå¥½è¯´çš„ï¼Œè°ƒç”¨äº†`advance_pos_nerf`ï¼Œä¸»è¦çœ‹è¿™ä¸ªï¼š

```cpp
__device__ void advance_pos_nerf(
	NerfPayload& payload,
	const BoundingBox& render_aabb,
	const mat3& render_aabb_to_local,
	const vec3& camera_fwd,
	const vec2& focal_length,
	uint32_t sample_index,
	const uint8_t* __restrict__ density_grid,
	uint32_t min_mip,
	uint32_t max_mip,
	float cone_angle_constant
) {
	if (!payload.alive) {
		return;
	}

	vec3 origin = payload.origin;
	vec3 dir = payload.dir;
	vec3 idir = vec3(1.0f) / dir;

	float cone_angle = calc_cone_angle(dot(dir, camera_fwd), focal_length, cone_angle_constant);

	float t = advance_n_steps(payload.t, cone_angle, ld_random_val(sample_index, payload.idx * 786433));
	t = if_unoccupied_advance_to_next_occupied_voxel(t, cone_angle, {origin, dir}, idir, density_grid, min_mip, max_mip, render_aabb, render_aabb_to_local);
	if (t >= MAX_DEPTH()) {
		payload.alive = false;
	} else {
		payload.t = t;
	}
}
```

ä¸Šé¢è¿™æ®µæ˜¯ray marchingçš„ä»£ç ï¼Œå…¶ä¸»è¦æ˜¯å¯¹`payload`å’Œ`density_grid`è¿›è¡Œæ“ä½œã€‚
ä»é€»è¾‘ä¸Šçœ‹æ˜¯æ ¹æ®`payload`ä¸­çš„å…‰çº¿èµ·ç‚¹å’Œæ–¹å‘å’Œ`density_grid`è®¡ç®—`if_unoccupied_advance_to_next_occupied_voxel`ï¼Œè®¡ç®—å‡ºæ¥`payload.t`ã€‚
é¡¾åæ€ä¹‰ï¼Œè¿™å°±æ˜¯åœ¨è®¡ç®—æ¯æ¡å…‰çº¿èƒ½è¡Œè¿›å¤šé•¿ã€‚

å…·ä½“çœ‹`if_unoccupied_advance_to_next_occupied_voxel`ï¼Œå°±æ˜¯ä¸€ä¸ªwhileå¾ªç¯ï¼Œåœ¨`density_grid`é‡Œé¢ä¸æ–­å‰è¿›ç›´åˆ°æ‰¾åˆ°ä¸€ä¸ªä¸é€æ˜çš„voxelæŒ¡ä½äº†è¿™æ¡å…‰çº¿ï¼š

```cpp
template <bool MIP_FROM_DT=false>
NGP_HOST_DEVICE float if_unoccupied_advance_to_next_occupied_voxel(
	float t,
	float cone_angle,
	const Ray& ray,
	const vec3& idir,
	const uint8_t* __restrict__ density_grid,
	uint32_t min_mip,
	uint32_t max_mip,
	BoundingBox aabb,
	mat3 aabb_to_local = mat3::identity()
) {
	while (true) {
		vec3 pos = ray(t);
		if (t >= MAX_DEPTH() || !aabb.contains(aabb_to_local * pos)) {
			return MAX_DEPTH();
		}

		uint32_t mip = clamp(MIP_FROM_DT ? mip_from_dt(calc_dt(t, cone_angle), pos) : mip_from_pos(pos), min_mip, max_mip);

		if (!density_grid || density_grid_occupied_at(pos, density_grid, mip)) {
			return t;
		}

		// Find largest empty voxel surrounding us, such that we can advance as far as possible in the next step.
		// Other places that do voxel stepping don't need this, because they don't rely on thread coherence as
		// much as this one here.
		while (mip < max_mip && !density_grid_occupied_at(pos, density_grid, mip+1)) {
			++mip;
		}

		t = advance_to_next_voxel(t, cone_angle, pos, ray.d, idir, mip);
	}
}
```

å…¶ä¸­çš„ç¬¬äºŒå±‚whileå¾ªç¯å’Œå›¾å½¢å­¦ä¸­çš„Mipmapæ¦‚å¿µæœ‰å…³ï¼Œç®€å•æ¥è¯´å°±æ˜¯è¿™ä¸ª`density_grid`æ˜¯åˆ†å±‚çš„ï¼Œmipè¶Šå¤§`density_grid`ä¸­çš„voxelè¦†ç›–èŒƒå›´è¶Šå¤§ã€‚
æ‰¾åˆ°äº†`density_grid`ä¸­å½“å‰ä½ç½®çš„æœ€å¤§çš„ç©ºvoxelåï¼Œå°±å¯ä»¥è°ƒç”¨`advance_to_next_voxel`å‰è¿›ä¸€æ­¥ã€‚
æ‰€ä»¥æ‰æœ‰æ³¨é‡Šé‡Œå†™çš„"Find largest empty voxel surrounding us, such that we can advance as far as possible in the next step."ã€‚

è¿™é‡Œç”¨äºåˆ¤æ–­é®æŒ¡çš„å‡½æ•°`density_grid_occupied_at`é•¿è¿™æ ·ï¼š

```cpp
inline NGP_HOST_DEVICE bool density_grid_occupied_at(const vec3& pos, const uint8_t* density_grid_bitfield, uint32_t mip) {
	uint32_t idx = cascaded_grid_idx_at(pos, mip);
	if (idx == 0xFFFFFFFF) {
		return false;
	}
	return density_grid_bitfield[idx/8+grid_mip_offset(mip)/8] & (1<<(idx%8));
}
```

å…¶ä¸­çš„ä¸¤ä¸ªæ ¸å¿ƒå‡½æ•°`cascaded_grid_idx_at`ç”¨äºè·å–`pos`æ‰€è¡¨ç¤ºçš„ç‚¹åœ¨mip levelå†…çš„å“ªä¸ªä½ç½®ã€`grid_mip_offset`ç”¨äºè·å–`mip`æ‰€è¡¨ç¤ºçš„mip levelä»å“ªé‡Œå¼€å§‹ï¼š

```cpp
inline NGP_HOST_DEVICE uint32_t cascaded_grid_idx_at(vec3 pos, uint32_t mip) {
	float mip_scale = scalbnf(1.0f, -mip); // 2^-mip
	pos -= vec3(0.5f); // ä»¥0.5ï¼Œ0.5ï¼Œ0.5ä¸ºä¸­å¿ƒç¼©æ”¾
	pos *= mip_scale; // mipè¶Šå¤§æ¯ä¸ªvoxelè¦†ç›–èŒƒå›´è¶Šå¤§æ‰€ä»¥æ˜¯ç¼©å°
	pos += vec3(0.5f); // æ‰€ä»¥ä¸Šè¿°æ“ä½œæ˜¯ä»¥0.5ï¼Œ0.5ï¼Œ0.5ä¸ºä¸­å¿ƒç¼©å°åæ ‡
	// ç»¼ä¸Šï¼Œåæ ‡çš„å–å€¼èŒƒå›´æ˜¯0åˆ°1ï¼Œè€Œä¸åŒçš„mipæ˜¯ä»¥0.5ï¼Œ0.5ï¼Œ0.5ä¸ºä¸­å¿ƒç¼©å°åæ ‡åå†æŸ¥è¯¢

	ivec3 i = pos * (float)NERF_GRIDSIZE(); // æ”¾å¤§åˆ°0åˆ°128ï¼Œè¿™é‡Œçš„ivec3é‡Œçš„xyzæ˜¯æ•´æ•°
	if (i.x < 0 || i.x >= NERF_GRIDSIZE() || i.y < 0 || i.y >= NERF_GRIDSIZE() || i.z < 0 || i.z >= NERF_GRIDSIZE()) {
		return 0xFFFFFFFF;
	}

	return morton3D(i.x, i.y, i.z); // è·å–è¯¥åæ ‡åœ¨morton3Dæ›²çº¿ä¸Šçš„ä½ç½®
	// å®˜æ–¹æ³¨é‡Šï¼šCalculates a 30-bit Morton code for the given 3D point located within the unit cube [0,1].
}

inline NGP_HOST_DEVICE uint32_t grid_mip_offset(uint32_t mip) {
	return NERF_GRID_N_CELLS() * mip; // æ¯ä¸ªmipè™½ç„¶å°ºåº¦ä¸ä¸€æ ·ï¼Œä½†æ˜¯å¯¹åº”çš„æ•°ç»„å¤§å°éƒ½æ˜¯ä¸€æ ·çš„
	// çœ‹ä¸Šé¢é‚£ä¸ªå‡½æ•°å°±èƒ½æ˜ç™½ï¼Œä¸åŒçš„mipæ˜¯ä»¥0.5ï¼Œ0.5ï¼Œ0.5ä¸ºä¸­å¿ƒç¼©å°åæ ‡åå†æŸ¥è¯¢ï¼Œæ‰€ä»¥å…«ä¸ªmipé‡Œé¢å…¶å®åˆ†åˆ«æ˜¯é è¿‘ä¸­å¿ƒåŒºåŸŸçš„åŠå¾„ä¸º2,4,6,8,16,32,64,128çš„æ–¹å—é‡Œæ˜¯æœ‰ç”¨å€¼ï¼Œå…¶ä»–åœ°æ–¹ç”¨ä¸ä¸Š
}
```

äºæ˜¯ï¼Œå¯¹äºæ¯ä¸ªè¾“å…¥åæ ‡ï¼Œ`advance_pos_nerf_kernel`ä»`density_grid_bitfield`é‡Œé¢æ‰¾åˆ°ä¸€ä¸ªæœ€è¿‘çš„é®æŒ¡å®ƒçš„åŒºåŸŸï¼Œå…¶è¿”å›çš„`payload.t`æ ‡å¿—äº†è¿™æ¡å…‰çº¿æœ€è¿‘çš„è¢«é®æŒ¡ç‚¹åœ¨ä½•å¤„ã€‚

### é™„åŠ çŸ¥è¯†ï¼š`density_grid`å’Œ`density_grid_bitfield`

æ³¨æ„å‰é¢æµ®ç‚¹å‹`density_grid`çš„å‚æ•°ä¼ åˆ°è¿™é‡Œå˜æˆäº†`density_grid_bitfield`é‡Œé¢å­˜çš„æ˜¯äºŒå€¼ğŸ˜‚ï¼Œå›å»å‰é¢æ‰¾æ‰¾å‘ç°`tracer.trace`çš„è¾“å…¥ç¡®å®æ˜¯`density_grid_bitfield`è€Œä¸æ˜¯æµ®ç‚¹å‹`density_grid`ã€‚
æ‰€ä»¥ç»§ç»­æ‰¾æ‰¾æµ®ç‚¹å‹`density_grid`æ˜¯æ€ä¹ˆå˜æˆ`density_grid_bitfield`çš„ï¼Œå‘ç°åœ¨è¿™é‡Œï¼š

```cpp
void Testbed::update_density_grid_mean_and_bitfield(cudaStream_t stream) {
	const uint32_t n_elements = NERF_GRID_N_CELLS();

	size_t size_including_mips = grid_mip_offset(NERF_CASCADES())/8;
	m_nerf.density_grid_bitfield.enlarge(size_including_mips);
	m_nerf.density_grid_mean.enlarge(reduce_sum_workspace_size(n_elements));

	CUDA_CHECK_THROW(cudaMemsetAsync(m_nerf.density_grid_mean.data(), 0, sizeof(float), stream));
	reduce_sum(m_nerf.density_grid.data(), [n_elements] __device__ (float val) { return fmaxf(val, 0.f) / (n_elements); }, m_nerf.density_grid_mean.data(), n_elements, stream);

	linear_kernel(grid_to_bitfield, 0, stream, n_elements/8 * NERF_CASCADES(), n_elements/8 * (m_nerf.max_cascade + 1), m_nerf.density_grid.data(), m_nerf.density_grid_bitfield.data(), m_nerf.density_grid_mean.data());

	for (uint32_t level = 1; level < NERF_CASCADES(); ++level) {
		linear_kernel(bitfield_max_pool, 0, stream, n_elements/64, m_nerf.get_density_grid_bitfield_mip(level-1), m_nerf.get_density_grid_bitfield_mip(level));
	}

	set_all_devices_dirty();
}

uint8_t* Testbed::Nerf::get_density_grid_bitfield_mip(uint32_t mip) {
	return density_grid_bitfield.data() + grid_mip_offset(mip)/8;
}
```

é‡Œé¢é¦–å…ˆè°ƒç”¨äº†ä¸€ä¸ª`grid_to_bitfield`å‡½æ•°ï¼š

```cpp
__global__ void grid_to_bitfield(
	const uint32_t n_elements,
	const uint32_t n_nonzero_elements,
	const float* __restrict__ grid,
	uint8_t* __restrict__ grid_bitfield,
	const float* __restrict__ mean_density_ptr
) {
	const uint32_t i = threadIdx.x + blockIdx.x * blockDim.x;
	if (i >= n_elements) return;
	if (i >= n_nonzero_elements) {
		grid_bitfield[i] = 0;
		return;
	}

	uint8_t bits = 0;

	float thresh = std::min(NERF_MIN_OPTICAL_THICKNESS(), *mean_density_ptr);

	NGP_PRAGMA_UNROLL
	for (uint8_t j = 0; j < 8; ++j) {
		bits |= grid[i*8+j] > thresh ? ((uint8_t)1 << j) : 0;
	}

	grid_bitfield[i] = bits;
}
```

åŸæ¥å°±æ˜¯å½“æµ®ç‚¹å‹`density_grid`ä¸­çš„æŸé¡¹è¶…è¿‡ä¸€ä¸ªé˜ˆå€¼å°±ç»™`density_grid_bitfield`å¯¹åº”çš„ä½ç½®1ã€‚
è¿™ä¸ªé˜ˆå€¼çš„å‡½æ•°`NERF_MIN_OPTICAL_THICKNESS`å°±è¿™æ ·ğŸ˜‚ï¼Œåæ­£å°±æ˜¯è¶…è¿‡0.01å°±è¡¨ç¤ºå¯è§ï¼š

```cpp
// Any alpha below this is considered "invisible" and is thus culled away.
inline constexpr __device__ float NERF_MIN_OPTICAL_THICKNESS() { return 0.01f; }
```

ç„¶åå¯¹æ¯ä¸€å±‚è°ƒç”¨`bitfield_max_pool`å‡½æ•°ï¼Œå¾ˆæ˜æ˜¾ï¼Œè¿™forå¾ªç¯é‡Œåˆæ˜¯max_poolåˆæ˜¯levelçš„ï¼Œè‚¯å®šå°±æ˜¯æ ¹æ®ç»†ç²’åº¦çš„`density_grid_bitfield`ç”Ÿæˆç²—ç²’åº¦çš„`density_grid_bitfield`ï¼Œå°±å¯¹åº”äºå‰é¢æåˆ°çš„Mipmapåˆ†å±‚density_gridã€‚
çœ‹çœ‹è¿™ä¸ª`bitfield_max_pool`æ€ä¹ˆä¸ªmax_poolæ³•ï¼š

```cpp
__global__ void bitfield_max_pool(const uint32_t n_elements,
	const uint8_t* __restrict__ prev_level,
	uint8_t* __restrict__ next_level
) {
	const uint32_t i = threadIdx.x + blockIdx.x * blockDim.x;
	if (i >= n_elements) return;

	uint8_t bits = 0;

	NGP_PRAGMA_UNROLL
	for (uint8_t j = 0; j < 8; ++j) {
		// If any bit is set in the previous level, set this
		// level's bit. (Max pooling.)
		bits |= prev_level[i*8+j] > 0 ? ((uint8_t)1 << j) : 0;
		// 3D Mortonæ›²çº¿æ¯8ä¸ªä¸ºä¸€ç»„æœ€å°å•å…ƒï¼ˆæƒ³è±¡å…«å‰æ ‘ï¼Œç±»ä¼¼ï¼‰ï¼Œ3D max poolingå½“å‰çº§çš„æ¯ä¸€ä¸ªæ–¹å—åˆéƒ½å¯¹åº”ä¸Šä¸€çº§çš„8ä¸ªæ–¹å—ï¼Œæ‰€ä»¥è¿™é‡Œç›´æ¥prev_level[i*8+j] > 0
		// æ¯ä¸ªbitsæœ‰8ä¸ªbitå¯¹åº”8ä¸ªå½“å‰çº§æ–¹å—ï¼Œæ‰€ä»¥forå¾ªç¯8æ¬¡å¡«æ»¡8ä¸ªbit
	}

	uint32_t x = morton3D_invert(i>>0) + NERF_GRIDSIZE()/8;
	uint32_t y = morton3D_invert(i>>1) + NERF_GRIDSIZE()/8;
	uint32_t z = morton3D_invert(i>>2) + NERF_GRIDSIZE()/8;

	next_level[morton3D(x, y, z)] |= bits;
}
```

äºæ˜¯å°±è¾¾åˆ°äº†åˆ†å±‚`density_grid_bitfield`çš„æ•ˆæœã€‚

## æ‰§è¡Œæ¸²æŸ“`tracer.trace`

```cpp
uint32_t Testbed::NerfTracer::trace(
	const std::shared_ptr<NerfNetwork<network_precision_t>>& network,
	const BoundingBox& render_aabb,
	const mat3& render_aabb_to_local,
	const BoundingBox& train_aabb,
	const vec2& focal_length,
	float cone_angle_constant,
	const uint8_t* grid,
	ERenderMode render_mode,
	const mat4x3 &camera_matrix,
	float depth_scale,
	int visualized_layer,
	int visualized_dim,
	ENerfActivation rgb_activation,
	ENerfActivation density_activation,
	int show_accel,
	uint32_t max_mip,
	float min_transmittance,
	float glow_y_cutoff,
	int glow_mode,
	const float* extra_dims_gpu,
	cudaStream_t stream
) {
	if (m_n_rays_initialized == 0) {
		return 0;
	}

	CUDA_CHECK_THROW(cudaMemsetAsync(m_hit_counter, 0, sizeof(uint32_t), stream));

	uint32_t n_alive = m_n_rays_initialized;
	// m_n_rays_initialized = 0;

	uint32_t i = 1;
	uint32_t double_buffer_index = 0;
	while (i < MARCH_ITER) {
		RaysNerfSoa& rays_current = m_rays[(double_buffer_index + 1) % 2];
		RaysNerfSoa& rays_tmp = m_rays[double_buffer_index % 2];
		++double_buffer_index;

		// Compact rays that did not diverge yet
		{
			CUDA_CHECK_THROW(cudaMemsetAsync(m_alive_counter, 0, sizeof(uint32_t), stream));
			linear_kernel(compact_kernel_nerf, 0, stream,
				n_alive,
				rays_tmp.rgba, rays_tmp.depth, rays_tmp.payload,
				rays_current.rgba, rays_current.depth, rays_current.payload,
				m_rays_hit.rgba, m_rays_hit.depth, m_rays_hit.payload,
				m_alive_counter, m_hit_counter
			);
			CUDA_CHECK_THROW(cudaMemcpyAsync(&n_alive, m_alive_counter, sizeof(uint32_t), cudaMemcpyDeviceToHost, stream));
			CUDA_CHECK_THROW(cudaStreamSynchronize(stream));
		}

		if (n_alive == 0) {
			break;
		}

		// Want a large number of queries to saturate the GPU and to ensure compaction doesn't happen toooo frequently.
		uint32_t target_n_queries = 2 * 1024 * 1024;
		uint32_t n_steps_between_compaction = clamp(target_n_queries / n_alive, (uint32_t)MIN_STEPS_INBETWEEN_COMPACTION, (uint32_t)MAX_STEPS_INBETWEEN_COMPACTION);

		uint32_t extra_stride = network->n_extra_dims() * sizeof(float);
		PitchedPtr<NerfCoordinate> input_data((NerfCoordinate*)m_network_input, 1, 0, extra_stride);
		linear_kernel(generate_next_nerf_network_inputs, 0, stream,
			n_alive,
			render_aabb,
			render_aabb_to_local,
			train_aabb,
			focal_length,
			camera_matrix[2],
			rays_current.payload,
			input_data,
			n_steps_between_compaction,
			grid,
			(show_accel>=0) ? show_accel : 0,
			max_mip,
			cone_angle_constant,
			extra_dims_gpu
		);
		uint32_t n_elements = next_multiple(n_alive * n_steps_between_compaction, BATCH_SIZE_GRANULARITY);
		GPUMatrix<float> positions_matrix((float*)m_network_input, (sizeof(NerfCoordinate) + extra_stride) / sizeof(float), n_elements);
		GPUMatrix<network_precision_t, RM> rgbsigma_matrix((network_precision_t*)m_network_output, network->padded_output_width(), n_elements);
		network->inference_mixed_precision(stream, positions_matrix, rgbsigma_matrix);

		if (render_mode == ERenderMode::Normals) {
			network->input_gradient(stream, 3, positions_matrix, positions_matrix);
		} else if (render_mode == ERenderMode::EncodingVis) {
			network->visualize_activation(stream, visualized_layer, visualized_dim, positions_matrix, positions_matrix);
		}

		linear_kernel(composite_kernel_nerf, 0, stream,
			n_alive,
			n_elements,
			i,
			train_aabb,
			glow_y_cutoff,
			glow_mode,
			camera_matrix,
			focal_length,
			depth_scale,
			rays_current.rgba,
			rays_current.depth,
			rays_current.payload,
			input_data,
			m_network_output,
			network->padded_output_width(),
			n_steps_between_compaction,
			render_mode,
			grid,
			rgb_activation,
			density_activation,
			show_accel,
			min_transmittance
		);

		i += n_steps_between_compaction;
	}

	uint32_t n_hit;
	CUDA_CHECK_THROW(cudaMemcpyAsync(&n_hit, m_hit_counter, sizeof(uint32_t), cudaMemcpyDeviceToHost, stream));
	CUDA_CHECK_THROW(cudaStreamSynchronize(stream));
	return n_hit;
}
```

æœ€æœ€æ ¸å¿ƒçš„NeRFæ¨æ–­è¿‡ç¨‹æ˜¯`network->inference_mixed_precision(stream, positions_matrix, rgbsigma_matrix);`ã€‚
æŒ‰ç…§NeRFçš„è¿è¡Œé€»è¾‘ï¼Œæ¨æ–­å‰çš„`compact_kernel_nerf`å’Œ`generate_next_nerf_network_inputs`å°±åº”è¯¥æ˜¯é‡‡æ ·è¿‡ç¨‹ï¼›
æ¨æ–­åçš„`composite_kernel_nerf`å°±åº”è¯¥æ˜¯ä½“æ¸²æŸ“è¿‡ç¨‹ã€‚

å†çœ‹å¤–é¢è¿™ä¸€ä¸ª`while (i < MARCH_ITER)`ï¼Œå“¦åŸæ¥æ˜¯ray marchingï¼Œæ‡‚äº†æ‡‚äº†ï¼Œä¸€æ­¥ä¸€æ­¥æ‰§è¡Œâ€œé‡‡æ ·->æ¨æ–­->ä½“æ¸²æŸ“â€è¿‡ç¨‹å‘—ã€‚

`compact_kernel_nerf`è¿™å‡½æ•°å¾ˆç®€å•ï¼Œå°±æ˜¯ray marchingæ¯ä¸€æ­¥å¼€å§‹æ—¶çš„åˆå§‹åŒ–è¿‡ç¨‹ï¼š

```cpp
__global__ void compact_kernel_nerf(
	const uint32_t n_elements,
	vec4* src_rgba, float* src_depth, NerfPayload* src_payloads,
	vec4* dst_rgba, float* dst_depth, NerfPayload* dst_payloads,
	vec4* dst_final_rgba, float* dst_final_depth, NerfPayload* dst_final_payloads,
	uint32_t* counter, uint32_t* finalCounter
) {
	const uint32_t i = threadIdx.x + blockIdx.x * blockDim.x;
	if (i >= n_elements) return;

	NerfPayload& src_payload = src_payloads[i];

	if (src_payload.alive) {
		uint32_t idx = atomicAdd(counter, 1);
		dst_payloads[idx] = src_payload;
		dst_rgba[idx] = src_rgba[i];
		dst_depth[idx] = src_depth[i];
	} else if (src_rgba[i].a > 0.001f) {
		uint32_t idx = atomicAdd(finalCounter, 1);
		dst_final_payloads[idx] = src_payload;
		dst_final_rgba[idx] = src_rgba[i];
		dst_final_depth[idx] = src_depth[i];
	}
}
```

è§‚å¯Ÿè¿™ä¸ªå‡½æ•°è°ƒç”¨çš„å‘¨å›´ï¼Œå¯ä»¥å‘ç°`compact_kernel_nerf`çš„è¾“å…¥æ¥è‡ªäº`m_rays`ï¼Œè¿™ä¸ª`m_rays`åœ¨`init_rays_from_camera`æœ«å°¾è¢«èµ‹å€¼ï¼Œå…¶å®šä¹‰ä¸º`RaysNerfSoa m_rays[2];`ï¼š

```cpp
struct RaysNerfSoa {
#if defined(__CUDACC__) || (defined(__clang__) && defined(__CUDA__))
	void copy_from_other_async(const RaysNerfSoa& other, cudaStream_t stream) {
		CUDA_CHECK_THROW(cudaMemcpyAsync(rgba, other.rgba, size * sizeof(vec4), cudaMemcpyDeviceToDevice, stream));
		CUDA_CHECK_THROW(cudaMemcpyAsync(depth, other.depth, size * sizeof(float), cudaMemcpyDeviceToDevice, stream));
		CUDA_CHECK_THROW(cudaMemcpyAsync(payload, other.payload, size * sizeof(NerfPayload), cudaMemcpyDeviceToDevice, stream));
	}
#endif

	void set(vec4* rgba, float* depth, NerfPayload* payload, size_t size) {
		this->rgba = rgba;
		this->depth = depth;
		this->payload = payload;
		this->size = size;
	}

	vec4* rgba;
	float* depth;
	NerfPayload* payload;
	size_t size;
};
```

å¾ˆæ˜æ˜¾ï¼Œ`m_rays`ç”¨äºåœ¨è¿™ä¸ªray marchingå¾ªç¯ä¸­äº¤æ›¿ä½¿ç”¨ï¼Œä¸€é¡¹å­˜å‚¨äº†å‰ä¸€æ¬¡çš„è®¡ç®—ç»“æœï¼Œå¦ä¸€é¡¹ç”¨äºå½“å‰è®¡ç®—ã€‚

### é‡‡æ ·`generate_next_nerf_network_inputs`

### ä½“æ¸²æŸ“`composite_kernel_nerf`