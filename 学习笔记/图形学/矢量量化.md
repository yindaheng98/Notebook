# 可微矢量量化

前置知识：[《一些常见的模型量化方法》](../人工智能/quant.md)

## (SIGGRAPH'22) Variable Bitrate Neural Fields (Vector-Quantized Auto-Decoder, VQ-AD)

![](i/20240423163529.png)

这论文把矢量量化的思想用到Feature Grid的训练和存储中，从而压缩模型的大小。
具体来说，本文的方法是把InstantNGP中使用空间hash函数查表的方法改成了一个可训练的indices。

前置知识：InstantNGP
* [《体渲染、体素、位置编码——主流Nerf质量优化和计算加速方法学习》](./Nerf加速.md)
* [《【转载】NeRF系列工作总结》](./NeRF系列工作总结.md)

在InstantNGP中，hash表的values可以视为矢量量化中的码矢，训练过程实际上就是在对这个码矢进行训练。
照这个思路，InstantNGP中codebook的key就是由空间hash函数生成的值。
但是，矢量量化的一个很重要的思想就是要对输入向量进行聚类，保证输入向量和其对应的码矢尽可能接近，而InstantNGP中feature值相近的region（内容相似的区域）经过空间hash出来的值随机均匀分布在指定范围内，于是相近的输入向量并不一定会对应到同一个码矢。

所以这样其实并不符合矢量量化的思想，矢量量化通常用聚类等方法让输入向量和其对应的码矢尽可能接近，在InstantNGP的场景就是内容相似的区域对应到同一个码矢。
本文就是要解决这个问题。

### 具体操作

符号|含义
-|-
$D\in\mathbb R^{2^b\times k}$|存储$2^b$个$k$维feature的codebook
$m$|空间等分为grid，grid每个顶点都对应一个feature，feature数量共$m$个
$V\in\mathbb Z^m$，取值范围$[0,2^b-1]$|grid的$m$个顶点对应的feature在codebook中的位置
$C\in\mathbb R^{m\times 2^b}$|softened indices矩阵，grid的$m$个顶点都能从其中查出一个$2^b$维向量

### 训练过程中的forward

1. （同InstantNGP）对于每个输入坐标，计算其在grid里的那个cube中，算出cube的8个顶点编号
2. 对于每个顶点，在$C$中查出$2^b$维向量，经过一个softmax激活函数
3. 把这个$2^b$维向量与$D$中的$2^b$个$k$维feature依次相乘后求和，作为这个顶点的feature
4. （同InstantNGP）插值、过MLP、渲染

这样，用可训练的softened indices矩阵$C$代替没法训练的hash函数indices，间接达到聚类的效果。

### 训练结束后的数据转换

softened indices矩阵$C$很大，显然不能直接存它，但是在前面训练的时候让它经过一个softmax激活函数就是为了让它里面的值变成one-hot的，于是直接$V=\text{argmax}_iC[i]$就把它转化为了一个$m$个$b$ bit整数的index表。

### 推断时的forward

1. （同InstantNGP）对于每个输入坐标，计算其在grid里的那个cube中，算出cube的8个顶点编号
2. 对于每个顶点，在$V$中查出$b$ bit整数即其feature在feature表中$D$的位置
3. 根据这个位置从$D$中找出这个顶点的feature
4. （同InstantNGP）插值、过MLP、渲染

### 压缩效果

原文中的描述：

>In contrast to using a hash function [Müller et al. 2022] for indexing, we need to store 𝑏-bit integers in the feature grid but we are able to use a much smaller codebook (table) due to the learned adaptivity of the indices.

比InstantNGP多存$m$个$b$ bit整数，空间划分越细$m$越大。

这种方法用可训练的softened indices间接达到聚类的效果，所以同样的codebook长度其质量要比用空间hash的InstantNGP更好。

![](i/20240423175518.png)

实验结果可以看出，VQ-AD主要的存储占用都是那$m$个$b$ bit整数，反倒是codebook成了大头，不过整体上还是VQ-AD更小。